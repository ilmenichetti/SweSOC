---
title: "Multi-model comparison"
author: "Lorenzo Menichetti"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aims of the project  
1. Produce aggregated national predictions on the C sequestration potential of Swedish agricultural soils with included model structural uncertainty  
2.  Understand the impact of various factors on prediction robustness 
To achieve these two goal, we will combine a selection of compartmental first-order SOC models, calibrate the whole ensemble on a set of Swedish long-term experiments, and then analyze the results of the ensemble in respect to various factors (for example climate, location, edaphic conditions).  
  
We will the utilize the ensemble for predictions, aggregated on a regional scale, about the total C sequestration potential in Swedish agricultural soils. These predictions will include a detailed uncertainty evaluation including also the model structural uncertainty.  


# Specific tasks

## 1 Data consolidation

## 2 Model comparison

## 2.1 Model description: similarities
All models are possible to be written as:  
\begin{equation}
I + \xi \cdot A \cdot C(t)
\end{equation}
Where $I$ is the inputs vector of lenght $n$, $\xi$ is the climate or edaphic scaling (either one scalar or a vector of length $n$), $A$ is the transfer matrix (square $n \times n$) and $C$ is the initialization vector of lenght $n$.  
All models are assumed as non-autonomous and have forcing variables.  
Different models will mainly differ in:
* size ($n$)
* transfer matrix $A$, which can have for example feedbacks, not all models are necessarily a cascade  
And in the ways they estimate their forcing variables:  
* in the way the estimate the inputs, usually variable over time
* in the way they estimate the climatic scaling, also variable over time  
All these differences will interact with each other. In order to test for the differences in model structures ($n$ and $A$) without depending on the forcing variable estimation approaches, we will implement the latter into the Bayesian probabilistic model for calibration. 
The term $\xi$ can be decomposed into separate temperature $\tau$ and moisture $\psi$ effects.

### 2.1.2 The reduction functions $\tau$ and $\xi$
These functions are all classifiable under the following three groups:  
*Linear:* representing the increase in activity with temperature or increase in activity with moisture in a more realistic way 
```{r echo=FALSE, out.width="30%"}
plot(seq(1:20), seq(1:20), type="l", xaxt="n", yaxt="n", ylab="", xlab="", col="blue")
```  

*Non-linear but monotonic:* representing the increase in activity with temperature or increase in activity with moisture in a more realistic way  
```{r echo=FALSE, out.width="30%"}
plot(seq(1:20), seq(1:20)^2, type="l", xaxt="n", yaxt="n", ylab="", xlab="", col="blue")
```  

*Non-linear and non-monotonic:* representing the increase in activity with temperature and subsequent decrease due to processes like protein denaturation, or increase in activity with moisture with subsequent decrease due to lack of oxygen  
```{r echo=FALSE, out.width="30%"}
p = seq(0,1, length=100)
plot(p, dbeta(p, 2, 2), type='l', xaxt="n", yaxt="n", ylab="", xlab="", col="blue")
```  


### 2.1.3 The input estimation functions


## 2.2 Bayesian testing framework
We will build one single Bayesian model for each decomposition model, which will include all the possible combinations of forcing variables. The graph of the probability model is represented below, where $\Theta$ represents the SOC decomposition model:  

```{tikz, echo=FALSE, fig.cap = "The core graph of the Bayesian model for each SOC decomposition model", fig.ext = 'png'}
\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning}
\tikzset{
    -Latex,auto,node distance =1 cm and 1 cm,semithick,
    state/.style ={ellipse, draw, minimum width = 0.7 cm,fill=teal!60},
    statedata/.style ={ellipse, draw, minimum width = 0.7 cm,fill=orange!60},
    statemin/.style ={ellipse, draw, minimum width = 0 cm},
    point/.style = {circle, draw, inner sep=0.04cm,fill,node contents={}},
    bidirected/.style={Latex-Latex,dashed},
    el/.style = {inner sep=2pt, align=left, sloped}
}
\begin{tikzpicture}
    \node[state] (1) {$\Theta$};
    \node[statedata] (2) [right =of 1, xshift=9cm] {$Data$};
    \node[state] (31) [above =of 1, xshift=2cm,yshift=2cm] {$\tau_1$};
    \node[state] (32) [above =of 1, xshift=3cm,yshift=2cm] {$\tau_2$};
    \node[state] (33) [above =of 1, xshift=4cm,yshift=2cm] {$\tau_3$};
    \node[statemin] (3x) [above =of 1, xshift=2cm,yshift=1cm] {};

    \node[state] (41) [above =of 1,  xshift=-2cm,yshift=2cm] {$\psi_1$};
    \node[state] (42) [above =of 1,  xshift=-3cm,yshift=2cm] {$\psi_2$};
    \node[state] (43) [above =of 1,  xshift=-4cm,yshift=2cm] {$\psi_3$};
    \node[statemin] (4x) [above =of 1, xshift=-2cm,yshift=1cm] {};

    \node[state] (51) [below =of 1, xshift=-2cm, yshift=-2cm] {$I_1$};
    \node[state] (52) [below =of 1, xshift=-3cm, yshift=-2cm] {$I_2$};
    \node[statemin] (5x) [below =of 1, xshift=-2cm,yshift=-1cm] {};

    \node[state] (53) [below =of 1,  xshift=2cm, yshift=-2cm] {$E_1$};
    \node[state] (54) [below =of 1,  xshift=3cm, yshift=-2cm] {$E_2$};
    \node[state] (55) [below =of 1,  xshift=4cm, yshift=-2cm] {$E_3$};
    \node[statemin] (55x) [below =of 1, xshift=2cm,yshift=-1cm] {};

    \path (1) edge node[above] {$p(\Theta  \mid \sum_{1}^{i} p(\tau_i) = 1, \sum_{1}^{i} p(\psi_i) = 1, \sum_{1}^{i} p(I_i) =1, \sum_{1}^{i} p(E_i)) =1 $} (2);

    \path (31) edge node[el,above] {} (3x);
    \path (32) edge node[el,above] {} (3x);
    \path (33) edge node[el,above] {} (3x);
    \path (3x) edge node[el,below, rotate=180] {$\sum_{1}^{i} p(\tau_i) = 1$} (1);
    \path (41) edge node[el,above] {} (4x);
    \path (42) edge node[el,above] {} (4x);
    \path (43) edge node[el,above] {} (4x);
    \path (4x) edge node[el,below] {$\sum_{1}^{i} p(\psi_i) = 1$} (1);

    \path (51) edge node[el,above] {} (5x);
    \path (52) edge node[el,above] {} (5x);
    \path (5x) edge node[el,below] {$\sum_{1}^{i} p(I_i) = 1$} (1);

    \path (53) edge node[el,above] {} (55x);
    \path (54) edge node[el,above] {} (55x);
    \path (55) edge node[el,above] {} (55x);
    \path (55x) edge node[el,below, rotate=180] {$\sum_{1}^{i} p(e_i) = 1$} (1);

\end{tikzpicture}

```



## 3 The models
The first step is defining what a specific model is.  
Here we will consider a SOC decomposition model as a compartmental model where compartments are in general connected with a cascade of mass fluxes and defined by a decomposition kinetic associated with each pool. The models which can present or not one or more inert pools, which are just a special case of pool with kinetic zero (and therefore present the peculiarity of simulate over time for SOC an asymptote translated above zero).  
Models can also present interactions between the pools, which makes them nonlinear. Here we define as linear a model which presents as variable only the pool masses. A nonlinear model presents other variables, usually the kinetics but in some cases something else like the input partitioning, which also vary as a function of the pools.

### 2.1 Linear models

#### 2.1.1 Models with an inert pool

#### 2.1.1 Models with no inert pool


### 2.2 Nonlinear models

### Autonomous and non autonomous models: the type of variance that they can describe


## 3 The climate scaling 
Compartmental models in general use a scaling factor, combination of soil moisture and temperature effects, to scale the decomposition kinetics of each pool.  
There are many different functions to express the impact of temperature and moisture on SOC decomposition, which can differ in their predictions. 
Each decomposition model will be associated to its own climate scaling functions during the calibration of the ensemble. 


## 4 Model calibration

### 4.1 Model initialization


### 4.2 The climate scaling effects
Since each model will be calibrated for all kinetic parameters, the original parameterization of each model will have no impact on the results. Model initialization will also have no impact, and therefore model performances will depend only on the decomposition model itself and on the climatic and edaphic scaling.  
For what concerns linear models, structures are all pretty similar as described in section [2 The models] and all define a combination of the same shape when integrated over time (which is an exponential decomposition curve). The amount of pools in a model defines how many of such shapes are combined together, varying the shape of the total SOC predicted but not substantially over the short time scale where we have data. The biggest difference is caused by the presence of very slow or inert pools, and it can become visible over very long term time scales.  
Over the relatively short (in terms of SOC kinetics) time scale of the measured data, the main differences in model fitness would therefore come from 


## 4.2 Standardizing the input estimation

## 5 Comparing models

## 6 Extrapolating national predictions from the ensemble

